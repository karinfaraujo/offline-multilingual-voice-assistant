# ğŸ™ï¸ Offline Multilingual Voice Assistant

**Local-first â€¢ Privacy-friendly â€¢ Fully Offline Voice AI**

---

## ğŸŒ Overview

**Offline Multilingual Voice Assistant** is a fully local, voice-based AI system that enables users to interact with a language model using **speech only**, without relying on any cloud APIs.

The assistant records user speech, transcribes it, generates a response using a **local Large Language Model**, and speaks the answer back â€” all **100% offline**, running entirely on the user's machine.

This project demonstrates a complete **end-to-end offline voice AI pipeline**, focused on privacy, modularity, and real-world usability.

---

## ğŸ¬ Demo

![Offline Multilingual Voice Assistant Demo ([https://github.com/karinfaraujo/offline-multilingual-voice-assistant/blob/main/offline-multilingual-voice-assistant/assets/demo.gif?raw=true])

---

## âœ¨ Features

- ğŸ¤ **Speech-to-Text (Offline)** using Faster-Whisper  
- ğŸ§  **Local Large Language Model** via Ollama  
- ğŸ”Š **Text-to-Speech (Offline)** with Coqui TTS  
- ğŸŒ Multilingual-ready pipeline  
- ğŸ” Privacy-first (no data leaves your machine)  
- ğŸ§© Modular, clean, and extensible architecture  

---

## ğŸ—ï¸ System Architecture

ğŸ™ï¸ User Voice  
â†“  
ğŸ¤ Audio Recorder  
â†“  
ğŸ§  Speech-to-Text (Whisper)  
â†“  
ğŸ’¬ Local LLM (Ollama)  
â†“  
ğŸ”Š Text-to-Speech  
â†“  
ğŸ—£ï¸ Spoken Response  

---

## ğŸ§° Tech Stack

| Layer            | Technology                |
|------------------|---------------------------|
| Language         | Python 3.10               |
| Speech-to-Text   | Faster-Whisper            |
| Language Model   | Ollama (LLaMA 3.x)        |
| Text-to-Speech   | Coqui TTS                 |
| Audio Processing | SoundDevice, NumPy, SciPy |

---

## ğŸ“ Project Structure

offline-multilingual-voice-assistant/  
â”œâ”€â”€ app/  
â”‚   â”œâ”€â”€ main.py  
â”‚   â”œâ”€â”€ audio_recorder.py  
â”‚   â”œâ”€â”€ speech_to_text.py  
â”‚   â”œâ”€â”€ llm.py  
â”‚   â””â”€â”€ text_to_speech.py  
â”‚  
â”œâ”€â”€ assets/  
â”‚   â””â”€â”€ demo.gif
â”‚  
â”œâ”€â”€ requirements.txt  
â”œâ”€â”€ .gitignore  
â””â”€â”€ README.md  

---

## â–¶ï¸ Getting Started

### 1ï¸âƒ£ Clone the repository

~~~bash
git clone https://github.com/karifaraujo/offline-multilingual-voice-assistant.git
cd offline-multilingual-voice-assistant
~~~

---

### 2ï¸âƒ£ Create and activate a virtual environment

~~~bash
python -m venv venv
source venv/Scripts/activate   # Windows (Git Bash)
~~~

---

### 3ï¸âƒ£ Install dependencies

~~~bash
pip install -r requirements.txt
~~~

---

### 4ï¸âƒ£ Install Ollama and a local model

Download Ollama from:  
https://ollama.com

~~~bash
ollama pull llama3.2:1b
~~~

---

### 5ï¸âƒ£ Run the assistant

~~~bash
python app/main.py
~~~

Speak into your microphone and wait for the spoken response.

---

## ğŸ” Why Offline?

Most voice assistants rely on cloud APIs, which introduce:

- High latency  
- Privacy concerns  
- Usage limits  
- API costs  

This project explores a **local-first AI approach**, making it ideal for environments where **privacy, control, and offline availability** are essential.

---

## â™¿ Accessibility & Use Cases

- Voice-based interfaces for visually impaired users  
- Assistants in offline or restricted environments  
- Educational demonstrations of AI pipelines  
- Foundations for avatars and sign-language systems  
- Edge AI and smart device applications  

---

## ğŸ”® Future Improvements

- Streaming responses to reduce perceived latency  
- Automatic language detection  
- Wake-word activation  
- GUI or web-based interface  
- Conversation memory and context  

---

## ğŸ‘©â€ğŸ’» Author

**Karin Araujo**  
Focused on data, AI, and applied machine learning projects.

---

## ğŸ“„ License

This project is intended for **educational and portfolio purposes**.
